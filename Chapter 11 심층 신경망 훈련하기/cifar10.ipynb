{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([100, 1, 10])\n",
      "tensor([[0.1002, 0.1055, 0.0962, 0.1043, 0.1052, 0.0951, 0.0942, 0.0967, 0.1161,\n",
      "         0.0865]], device='cuda:0')\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "num_epochs = 10 ** 10\n",
    "batch_size = 32\n",
    "path = \"./cifar10_model.pt\"\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience=2, save_path=path):\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.min_loss = float(\"inf\")\n",
    "        self.count = 0\n",
    "    \n",
    "    def should_stop(self, model, loss):\n",
    "        if loss < self.min_loss:\n",
    "            self.min_loss = loss\n",
    "            self.count = 0\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "        elif loss > self.min_loss:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def load(self, model):\n",
    "        model.load_state_dict(torch.load(self.save_path))\n",
    "early_stopper = EarlyStopping(patience=3)\n",
    "\n",
    "train_data = datasets.CIFAR10(\"./\", download=True, train=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.CIFAR10(\"./\", download=True, train=False, transform=transforms.ToTensor())\n",
    "train = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def lecun_normal_(tensor):\n",
    "    input_size = tensor.shape[-1]\n",
    "    std = math.sqrt(1 / input_size)\n",
    "    with torch.no_grad():\n",
    "        tensor.normal_(-std, std)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input = self.he_initialize(nn.Linear(3 * 32 * 32, 100))\n",
    "        self.linear_layers = nn.ModuleList([self.he_initialize(nn.Linear(100, 100), nonlinearity=\"linear\") for _ in range(19)])\n",
    "        self.alpha_dropouts = nn.ModuleList([nn.AlphaDropout() for _ in range(19)])\n",
    "        self.output = self.he_initialize(nn.Linear(100, 10))\n",
    "    \n",
    "    def he_initialize(self, layer, nonlinearity=\"leaky_relu\"):\n",
    "        nn.init.kaiming_normal_(layer.weight, nonlinearity=nonlinearity)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "        return layer\n",
    "    \n",
    "    #apply BN before activatioion function, excepts output layer\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.elu(self.input(x))\n",
    "        #for linear in self.linear_layers:\n",
    "        for linear, dropout in zip(self.linear_layers, self.alpha_dropouts):\n",
    "            x = F.selu(linear(x))\n",
    "            x = dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "model = MyModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train), epochs=num_epochs)\n",
    "\n",
    "'''\n",
    "print(f\"Train on {len(train_data)}, test on {len(test_data)} samples\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    accuracy = 0\n",
    "    for datas, labels in train:\n",
    "        datas = datas.cuda()\n",
    "        labels = labels.cuda()\n",
    "        result = model(datas)\n",
    "        loss = criterion(result, labels)\n",
    "        correct = torch.sum(result.argmax(dim=1) == labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy += correct.item()\n",
    "        total_loss += loss.item()\n",
    "    writer.add_scalar(\"Loss/train\", total_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", total_loss, epoch)\n",
    "    print(f\"Epoch {epoch + 1}, loss: {total_loss / len(train)}, accuracy: {accuracy / len(train_data)}\")\n",
    "    if early_stopper.should_stop(model, total_loss):\n",
    "        print(f\"EarlyStopping: [Epoch: {epoch - early_stopper.count}]\")\n",
    "        break\n",
    "'''\n",
    "writer.close()\n",
    "early_stopper.load(model)\n",
    "\n",
    "'''\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    accuracy = 0\n",
    "    for datas, labels in test:\n",
    "        datas = datas.cuda()\n",
    "        labels = labels.cuda()\n",
    "        result = model(datas)\n",
    "        loss = criterion(result, labels)\n",
    "        correct = torch.sum(result.argmax(dim=1) == labels)\n",
    "        accuracy += correct.item()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"test_loss: {total_loss / len(test)}, test_accuracy: {accuracy / len(test_data)}\")\n",
    "'''\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.train()\n",
    "\n",
    "    result = model(test_data[0][0].unsqueeze(0).cuda())\n",
    "    stacked = torch.stack([F.softmax(model(test_data[0][0].unsqueeze(0).cuda()), dim=-1) for _ in range(100)])\n",
    "    print(stacked.size())\n",
    "    print(stacked.mean(dim=0))\n",
    "    print(test_data[0][1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
