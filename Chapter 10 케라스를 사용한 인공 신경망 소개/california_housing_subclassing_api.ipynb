{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 5.918081283569336 valid_loss: 5.738646507263184\n",
      "Epoch 2, train_loss: 5.914943218231201 valid_loss: 5.735588073730469\n",
      "Epoch 3, train_loss: 5.911807060241699 valid_loss: 5.732532978057861\n",
      "Epoch 4, train_loss: 5.908674240112305 valid_loss: 5.729480743408203\n",
      "Epoch 5, train_loss: 5.905543804168701 valid_loss: 5.7264299392700195\n",
      "Epoch 6, train_loss: 5.902416229248047 valid_loss: 5.723382472991943\n",
      "Epoch 7, train_loss: 5.8992919921875 valid_loss: 5.720338821411133\n",
      "Epoch 8, train_loss: 5.896169662475586 valid_loss: 5.717297554016113\n",
      "Epoch 9, train_loss: 5.893051624298096 valid_loss: 5.714259147644043\n",
      "Epoch 10, train_loss: 5.889935493469238 valid_loss: 5.711223125457764\n",
      "Epoch 11, train_loss: 5.88682222366333 valid_loss: 5.708190441131592\n",
      "Epoch 12, train_loss: 5.883710861206055 valid_loss: 5.705160140991211\n",
      "Epoch 13, train_loss: 5.880603790283203 valid_loss: 5.702131748199463\n",
      "Epoch 14, train_loss: 5.877500534057617 valid_loss: 5.699107646942139\n",
      "Epoch 15, train_loss: 5.8743977546691895 valid_loss: 5.6960859298706055\n",
      "Epoch 16, train_loss: 5.871298789978027 valid_loss: 5.693066596984863\n",
      "Epoch 17, train_loss: 5.868202209472656 valid_loss: 5.690049648284912\n",
      "Epoch 18, train_loss: 5.865108966827393 valid_loss: 5.687036514282227\n",
      "Epoch 19, train_loss: 5.86201810836792 valid_loss: 5.684024810791016\n",
      "Epoch 20, train_loss: 5.858930587768555 valid_loss: 5.681016445159912\n",
      "test_loss: 5.685091972351074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#learn scaling parameters(mean and std) on the train data\n",
    "X_train = torch.FloatTensor(scaler.fit_transform(X_train)).cuda()\n",
    "#use learned scaling parameters\n",
    "X_valid = torch.FloatTensor(scaler.transform(X_valid)).cuda()\n",
    "X_test = torch.FloatTensor(scaler.transform(X_test)).cuda()\n",
    "\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(1).cuda()\n",
    "y_valid = torch.FloatTensor(y_valid).unsqueeze(1).cuda()\n",
    "y_test = torch.FloatTensor(y_test).unsqueeze(1).cuda()\n",
    "\n",
    "class WideAndDeepModel(nn.Module):\n",
    "    def __init__(self, units=30):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(6, units)\n",
    "        self.hidden2 = nn.Linear(units, units)\n",
    "        self.main_output = nn.Linear(35, 1)\n",
    "        self.aux_output = nn.Linear(units, 1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = F.relu(self.hidden1(input_B))\n",
    "        hidden2 = F.relu(self.hidden2(hidden1))\n",
    "        concat = torch.cat((input_A, hidden2), dim=-1)\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "model = WideAndDeepModel().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1e-3)\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    main_output, aux_output = model((X_train_A, X_train_B))\n",
    "    loss = 0.9 * criterion(main_output, y_train) + 0.1 * criterion(aux_output, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss = loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, train_loss: {total_loss}\", end=\" \")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        main_output, aux_output = model((X_valid_A, X_valid_B))\n",
    "        loss = 0.9 * criterion(main_output, y_valid) + 0.1 * criterion(aux_output, y_valid)\n",
    "        total_loss = loss.item()\n",
    "        print(f\"valid_loss: {total_loss}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    main_output, aux_output = model((X_test_A, X_test_B))\n",
    "    loss = 0.9 * criterion(main_output, y_test) + 0.1 * criterion(aux_output, y_test)\n",
    "    total_loss = loss.item()\n",
    "    print(f\"test_loss: {total_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
